<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 More on Hierarchical Linear Models | Generalized Linear Mixed Models with R: A tutorial</title>
  <meta name="description" content="8 More on Hierarchical Linear Models | Generalized Linear Mixed Models with R: A tutorial" />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="8 More on Hierarchical Linear Models | Generalized Linear Mixed Models with R: A tutorial" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 More on Hierarchical Linear Models | Generalized Linear Mixed Models with R: A tutorial" />
  
  
  

<meta name="author" content="Rick Hass" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mixed-models-as-hierarchical-linear-models.html"/>
<link rel="next" href="using-lme4-to-fit-mlm-to-longitudinal-data.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="1" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>1</b> R Basics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-basics.html"><a href="r-basics.html#what-is-coding"><i class="fa fa-check"></i><b>1.1</b> What is “coding”</a></li>
<li class="chapter" data-level="1.2" data-path="r-basics.html"><a href="r-basics.html#r-console-versus-the-rstudio-script"><i class="fa fa-check"></i><b>1.2</b> R Console versus the RStudio Script</a></li>
<li class="chapter" data-level="1.3" data-path="r-basics.html"><a href="r-basics.html#working-with-data"><i class="fa fa-check"></i><b>1.3</b> Working with data</a></li>
<li class="chapter" data-level="1.4" data-path="r-basics.html"><a href="r-basics.html#using-a-script-to-generate-data-and-computing-the-mean-and-standard-deviation"><i class="fa fa-check"></i><b>1.4</b> Using a script to generate data, and computing the mean and standard deviation</a></li>
<li class="chapter" data-level="1.5" data-path="r-basics.html"><a href="r-basics.html#importing-data"><i class="fa fa-check"></i><b>1.5</b> Importing Data</a></li>
<li class="chapter" data-level="1.6" data-path="r-basics.html"><a href="r-basics.html#reading-in-an-spss-file"><i class="fa fa-check"></i><b>1.6</b> Reading in an SPSS file</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="r-basics.html"><a href="r-basics.html#a-quick-note-on-indexing"><i class="fa fa-check"></i><b>1.6.1</b> A quick note on indexing</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="r-basics.html"><a href="r-basics.html#frequencies-and-contingency-tables"><i class="fa fa-check"></i><b>1.7</b> Frequencies and Contingency Tables</a></li>
<li class="chapter" data-level="1.8" data-path="r-basics.html"><a href="r-basics.html#descriptive-statistics"><i class="fa fa-check"></i><b>1.8</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="r-basics.html"><a href="r-basics.html#the-describe-function"><i class="fa fa-check"></i><b>1.8.1</b> The describe function</a></li>
<li class="chapter" data-level="1.8.2" data-path="r-basics.html"><a href="r-basics.html#the-describeby-function"><i class="fa fa-check"></i><b>1.8.2</b> The describeBy function</a></li>
<li class="chapter" data-level="1.8.3" data-path="r-basics.html"><a href="r-basics.html#computing-a-new-variable-and-adding-it-to-the-dataframe"><i class="fa fa-check"></i><b>1.8.3</b> Computing a new variable and adding it to the dataframe</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="r-basics.html"><a href="r-basics.html#visualizing-data"><i class="fa fa-check"></i><b>1.9</b> Visualizing Data</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="r-basics.html"><a href="r-basics.html#histograms-and-density-plots"><i class="fa fa-check"></i><b>1.9.1</b> Histograms and density plots</a></li>
<li class="chapter" data-level="1.9.2" data-path="r-basics.html"><a href="r-basics.html#boxplots"><i class="fa fa-check"></i><b>1.9.2</b> Boxplots</a></li>
<li class="chapter" data-level="1.9.3" data-path="r-basics.html"><a href="r-basics.html#scatterplots"><i class="fa fa-check"></i><b>1.9.3</b> Scatterplots</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="r-basics.html"><a href="r-basics.html#review"><i class="fa fa-check"></i><b>1.10</b> Review</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Ordinary Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#our-data"><i class="fa fa-check"></i><b>2.1</b> Our data</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#converting-from-numeric-to-factor"><i class="fa fa-check"></i><b>2.1.1</b> Converting from numeric to factor</a></li>
<li class="chapter" data-level="2.1.2" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#confidence-intervals-v.-p-values"><i class="fa fa-check"></i><b>2.1.2</b> Confidence Intervals v. p-values</a></li>
<li class="chapter" data-level="2.1.3" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#adding-interaction-terms"><i class="fa fa-check"></i><b>2.1.3</b> Adding interaction terms</a></li>
<li class="chapter" data-level="2.1.4" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#interaction-model-for-gambling"><i class="fa fa-check"></i><b>2.1.4</b> Interaction model for Gambling</a></li>
<li class="chapter" data-level="2.1.5" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#interaction-output"><i class="fa fa-check"></i><b>2.1.5</b> Interaction Output</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#anova-and-drop1-commands"><i class="fa fa-check"></i><b>2.2</b> ANOVA and drop1 commands</a></li>
<li class="chapter" data-level="2.3" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#plots"><i class="fa fa-check"></i><b>2.3</b> Plots</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>3</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#an-example-diabetes-risk-factors"><i class="fa fa-check"></i><b>3.1</b> An Example: Diabetes risk factors</a></li>
<li class="chapter" data-level="3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#model-discrimination-and-fit"><i class="fa fa-check"></i><b>3.2</b> Model Discrimination and Fit</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#roc-and-auroc"><i class="fa fa-check"></i><b>3.2.1</b> ROC and AUROC</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regression-with-counts-as-outcomes.html"><a href="regression-with-counts-as-outcomes.html"><i class="fa fa-check"></i><b>4</b> Regression with Counts as Outcomes</a>
<ul>
<li class="chapter" data-level="4.1" data-path="regression-with-counts-as-outcomes.html"><a href="regression-with-counts-as-outcomes.html#example-data-phmc-fruits-question"><i class="fa fa-check"></i><b>4.1</b> Example data: PHMC Fruits question</a></li>
<li class="chapter" data-level="4.2" data-path="regression-with-counts-as-outcomes.html"><a href="regression-with-counts-as-outcomes.html#coefficients-in-poisson-models"><i class="fa fa-check"></i><b>4.2</b> Coefficients in Poisson Models</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="regression-with-counts-as-outcomes.html"><a href="regression-with-counts-as-outcomes.html#exponentiated-coefficients-interpretation"><i class="fa fa-check"></i><b>4.2.1</b> Exponentiated Coefficients Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="regression-with-counts-as-outcomes.html"><a href="regression-with-counts-as-outcomes.html#overdispersion-and-other-count-regression-models"><i class="fa fa-check"></i><b>4.3</b> Overdispersion and other count regression models</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="regression-with-counts-as-outcomes.html"><a href="regression-with-counts-as-outcomes.html#overdispersion-and-zero-inflation"><i class="fa fa-check"></i><b>4.3.1</b> Overdispersion and Zero-inflation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="regression-with-counts-as-outcomes.html"><a href="regression-with-counts-as-outcomes.html#zero-inflated-models"><i class="fa fa-check"></i><b>4.4</b> Zero-inflated models</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-fit-and-assumptions-for-glms.html"><a href="model-fit-and-assumptions-for-glms.html"><i class="fa fa-check"></i><b>5</b> Model Fit and Assumptions for GLMs</a>
<ul>
<li class="chapter" data-level="5.1" data-path="model-fit-and-assumptions-for-glms.html"><a href="model-fit-and-assumptions-for-glms.html#model-checking-and-diagnostics-in-ordinary-least-squares"><i class="fa fa-check"></i><b>5.1</b> Model Checking and Diagnostics in Ordinary Least Squares</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="model-fit-and-assumptions-for-glms.html"><a href="model-fit-and-assumptions-for-glms.html#residuals-v.-fitted-values"><i class="fa fa-check"></i><b>5.1.1</b> Residuals v. Fitted Values</a></li>
<li class="chapter" data-level="5.1.2" data-path="model-fit-and-assumptions-for-glms.html"><a href="model-fit-and-assumptions-for-glms.html#normalty-of-residuals"><i class="fa fa-check"></i><b>5.1.2</b> Normalty of residuals</a></li>
<li class="chapter" data-level="5.1.3" data-path="model-fit-and-assumptions-for-glms.html"><a href="model-fit-and-assumptions-for-glms.html#leverage"><i class="fa fa-check"></i><b>5.1.3</b> Leverage</a></li>
<li class="chapter" data-level="5.1.4" data-path="model-fit-and-assumptions-for-glms.html"><a href="model-fit-and-assumptions-for-glms.html#influence"><i class="fa fa-check"></i><b>5.1.4</b> Influence</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="model-fit-and-assumptions-for-glms.html"><a href="model-fit-and-assumptions-for-glms.html#model-checking-and-diagnostics-in-logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Model Checking and Diagnostics in Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="model-fit-and-assumptions-for-glms.html"><a href="model-fit-and-assumptions-for-glms.html#hosmer-lemeshow-test"><i class="fa fa-check"></i><b>5.2.1</b> Hosmer-Lemeshow Test</a></li>
<li class="chapter" data-level="5.2.2" data-path="model-fit-and-assumptions-for-glms.html"><a href="model-fit-and-assumptions-for-glms.html#nagelkerkes-pseudo-r-squared"><i class="fa fa-check"></i><b>5.2.2</b> Nagelkerke’s pseudo R-squared</a></li>
<li class="chapter" data-level="5.2.3" data-path="model-fit-and-assumptions-for-glms.html"><a href="model-fit-and-assumptions-for-glms.html#residuals-and-leverages"><i class="fa fa-check"></i><b>5.2.3</b> Residuals and leverages</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="model-fit-and-assumptions-for-glms.html"><a href="model-fit-and-assumptions-for-glms.html#summary"><i class="fa fa-check"></i><b>5.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introducing-mixed-models-and-the-lme4-package.html"><a href="introducing-mixed-models-and-the-lme4-package.html"><i class="fa fa-check"></i><b>6</b> Introducing mixed-models and the lme4 package</a>
<ul>
<li class="chapter" data-level="6.1" data-path="introducing-mixed-models-and-the-lme4-package.html"><a href="introducing-mixed-models-and-the-lme4-package.html#options-in-r"><i class="fa fa-check"></i><b>6.1</b> Options in R</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="introducing-mixed-models-and-the-lme4-package.html"><a href="introducing-mixed-models-and-the-lme4-package.html#why-lme4"><i class="fa fa-check"></i><b>6.1.1</b> Why lme4?</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="introducing-mixed-models-and-the-lme4-package.html"><a href="introducing-mixed-models-and-the-lme4-package.html#a-possible-workflow"><i class="fa fa-check"></i><b>6.2</b> A possible workflow</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="introducing-mixed-models-and-the-lme4-package.html"><a href="introducing-mixed-models-and-the-lme4-package.html#high-school-and-beyond"><i class="fa fa-check"></i><b>6.2.1</b> High School and Beyond</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="introducing-mixed-models-and-the-lme4-package.html"><a href="introducing-mixed-models-and-the-lme4-package.html#model-1-one-way-random-effects-anova-using-reml"><i class="fa fa-check"></i><b>6.3</b> Model 1: one-way random-effects ANOVA using REML</a></li>
<li class="chapter" data-level="6.4" data-path="introducing-mixed-models-and-the-lme4-package.html"><a href="introducing-mixed-models-and-the-lme4-package.html#quasi-anova-estimator"><i class="fa fa-check"></i><b>6.4</b> Quasi-ANOVA estimator</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="introducing-mixed-models-and-the-lme4-package.html"><a href="introducing-mixed-models-and-the-lme4-package.html#unrestricted-maximum-likelihood"><i class="fa fa-check"></i><b>6.4.1</b> (unrestricted) Maximum Likelihood</a></li>
<li class="chapter" data-level="6.4.2" data-path="introducing-mixed-models-and-the-lme4-package.html"><a href="introducing-mixed-models-and-the-lme4-package.html#intraclass-correlation"><i class="fa fa-check"></i><b>6.4.2</b> Intraclass correlation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="introducing-mixed-models-and-the-lme4-package.html"><a href="introducing-mixed-models-and-the-lme4-package.html#bootstrapped-confidence-intervals"><i class="fa fa-check"></i><b>6.5</b> Bootstrapped confidence intervals</a></li>
<li class="chapter" data-level="6.6" data-path="introducing-mixed-models-and-the-lme4-package.html"><a href="introducing-mixed-models-and-the-lme4-package.html#linear-mixed-effects"><i class="fa fa-check"></i><b>6.6</b> Linear mixed-effects</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="introducing-mixed-models-and-the-lme4-package.html"><a href="introducing-mixed-models-and-the-lme4-package.html#random-effects-estimates"><i class="fa fa-check"></i><b>6.6.1</b> Random effects estimates</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="mixed-models-as-hierarchical-linear-models.html"><a href="mixed-models-as-hierarchical-linear-models.html"><i class="fa fa-check"></i><b>7</b> Mixed models as Hierarchical Linear Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="mixed-models-as-hierarchical-linear-models.html"><a href="mixed-models-as-hierarchical-linear-models.html#high-school-and-beyond-data"><i class="fa fa-check"></i><b>7.1</b> High School and Beyond Data</a></li>
<li class="chapter" data-level="7.2" data-path="mixed-models-as-hierarchical-linear-models.html"><a href="mixed-models-as-hierarchical-linear-models.html#model-1-unconditional-model"><i class="fa fa-check"></i><b>7.2</b> Model 1: unconditional model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="mixed-models-as-hierarchical-linear-models.html"><a href="mixed-models-as-hierarchical-linear-models.html#unconditional-icc"><i class="fa fa-check"></i><b>7.2.1</b> Unconditional ICC</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="mixed-models-as-hierarchical-linear-models.html"><a href="mixed-models-as-hierarchical-linear-models.html#means-as-outcomes-binary-predictor"><i class="fa fa-check"></i><b>7.3</b> Means as outcomes: Binary Predictor</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="mixed-models-as-hierarchical-linear-models.html"><a href="mixed-models-as-hierarchical-linear-models.html#means-as-outcomes-continuous-predictor"><i class="fa fa-check"></i><b>7.3.1</b> Means as outcomes: continuous predictor</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="mixed-models-as-hierarchical-linear-models.html"><a href="mixed-models-as-hierarchical-linear-models.html#multiple-hierarchical-linear-regression"><i class="fa fa-check"></i><b>7.4</b> Multiple hierarchical linear regression</a></li>
<li class="chapter" data-level="7.5" data-path="mixed-models-as-hierarchical-linear-models.html"><a href="mixed-models-as-hierarchical-linear-models.html#plotting-predictor-effects"><i class="fa fa-check"></i><b>7.5</b> Plotting predictor effects</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html"><i class="fa fa-check"></i><b>8</b> More on Hierarchical Linear Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#overview-1"><i class="fa fa-check"></i><b>8.1</b> Overview</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#load-the-data"><i class="fa fa-check"></i><b>8.1.1</b> Load the data</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#random-coefficients-model"><i class="fa fa-check"></i><b>8.2</b> Random Coefficients model</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#visualizing-the-within-school-slopes"><i class="fa fa-check"></i><b>8.2.1</b> Visualizing the within-school slopes</a></li>
<li class="chapter" data-level="8.2.2" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#fitting-the-random-coefficients-2-level-model"><i class="fa fa-check"></i><b>8.2.2</b> Fitting the Random Coefficients 2-level model</a></li>
<li class="chapter" data-level="8.2.3" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#adding-additional-level-1-predictors-to-the-random-coefficients-model"><i class="fa fa-check"></i><b>8.2.3</b> Adding additional level-1 predictors to the random coefficients model</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#intercepts-and-slopes-as-outcomes"><i class="fa fa-check"></i><b>8.3</b> Intercepts and Slopes as Outcomes</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#interactions-and-the-iso-model"><i class="fa fa-check"></i><b>8.3.1</b> Interactions and the ISO model</a></li>
<li class="chapter" data-level="8.3.2" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#mean-ses-and-student-ses"><i class="fa fa-check"></i><b>8.3.2</b> Mean SES and Student SES</a></li>
<li class="chapter" data-level="8.3.3" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#sector-and-student-ses"><i class="fa fa-check"></i><b>8.3.3</b> Sector and Student SES</a></li>
<li class="chapter" data-level="8.3.4" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#visualizing-the-mean-ses-effect"><i class="fa fa-check"></i><b>8.3.4</b> Visualizing the mean SES effect</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#model-comparison-with-likelihood-ratios-fixed-and-random"><i class="fa fa-check"></i><b>8.4</b> Model comparison with Likelihood Ratios: Fixed and Random</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#example-1-lrt-for-fixed-effects"><i class="fa fa-check"></i><b>8.4.1</b> Example 1: LRT for fixed effects</a></li>
<li class="chapter" data-level="8.4.2" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#example-2-lrt-for-random-effects"><i class="fa fa-check"></i><b>8.4.2</b> Example 2: LRT for random effects</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="more-on-hierarchical-linear-models.html"><a href="more-on-hierarchical-linear-models.html#diagnostics"><i class="fa fa-check"></i><b>8.5</b> Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html"><i class="fa fa-check"></i><b>9</b> Using lme4 to fit MLM to longitudinal data</a>
<ul>
<li class="chapter" data-level="9.1" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#data-description-average-reaction-time-per-day-for-subjects-in-a-sleep-deprivation-study-belenky-et-al.-2003"><i class="fa fa-check"></i><b>9.1</b> Data description: average reaction time per day for subjects in a sleep deprivation study (Belenky et al. 2003)</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#visualization-of-the-data"><i class="fa fa-check"></i><b>9.1.1</b> Visualization of the data</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#fitting-the-level-1-model"><i class="fa fa-check"></i><b>9.2</b> Fitting the level-1 model</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#model-equations"><i class="fa fa-check"></i><b>9.2.1</b> Model Equations</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#model-building"><i class="fa fa-check"></i><b>9.3</b> Model building</a></li>
<li class="chapter" data-level="9.4" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#model-comparison-and-testing"><i class="fa fa-check"></i><b>9.4</b> Model comparison and testing</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#option-1-using-confidence-intervals"><i class="fa fa-check"></i><b>9.4.1</b> Option 1: using confidence intervals</a></li>
<li class="chapter" data-level="9.4.2" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#tests"><i class="fa fa-check"></i><b>9.4.2</b> Tests</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#quadratic-model-with-lmer"><i class="fa fa-check"></i><b>9.5</b> Quadratic model with lmer</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#a-note-on-quadratic-terms"><i class="fa fa-check"></i><b>9.5.1</b> A note on quadratic terms</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#summary-of-the-sleepstudy-analysis"><i class="fa fa-check"></i><b>9.6</b> Summary of the sleepstudy analysis</a></li>
<li class="chapter" data-level="9.7" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#preview-of-the-blackmore-data-used-for-assignment-7b"><i class="fa fa-check"></i><b>9.7</b> Preview of the Blackmore data used for Assignment 7b</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="using-lme4-to-fit-mlm-to-longitudinal-data.html"><a href="using-lme4-to-fit-mlm-to-longitudinal-data.html#a-note-on-the-age-variable-in-blackmore"><i class="fa fa-check"></i><b>9.7.1</b> A note on the age variable in Blackmore</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html"><i class="fa fa-check"></i><b>10</b> Longitudinal models with upper-level predictors</a>
<ul>
<li class="chapter" data-level="10.1" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#overview-2"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#part-1-intercepts-and-slopes-as-outcomes"><i class="fa fa-check"></i><b>10.2</b> Part 1: Intercepts and Slopes as Outcomes</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#a-short-tutorial-on-ggplot2"><i class="fa fa-check"></i><b>10.2.1</b> A short tutorial on ggplot2</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#adding-level-2-predictors"><i class="fa fa-check"></i><b>10.3</b> Adding level 2 predictors</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#transforming-the-exercise-variable"><i class="fa fa-check"></i><b>10.3.1</b> Transforming the Exercise Variable</a></li>
<li class="chapter" data-level="10.3.2" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#steps-1-and-2"><i class="fa fa-check"></i><b>10.3.2</b> Steps 1 and 2:</a></li>
<li class="chapter" data-level="10.3.3" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#step-3"><i class="fa fa-check"></i><b>10.3.3</b> Step 3:</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#summary-1"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
<li class="chapter" data-level="10.5" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#adding-level-2-explanatory-variables"><i class="fa fa-check"></i><b>10.5</b> Adding Level-2 Explanatory variables</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#visualizing-the-transformed-data"><i class="fa fa-check"></i><b>10.5.1</b> Visualizing the transformed data</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#time-dependent-covariates"><i class="fa fa-check"></i><b>10.6</b> Time dependent covariates</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#level-1-time-dependent-sports-predictor"><i class="fa fa-check"></i><b>10.6.1</b> Level-1: Time-dependent sports predictor</a></li>
<li class="chapter" data-level="10.6.2" data-path="longitudinal-models-with-upper-level-predictors.html"><a href="longitudinal-models-with-upper-level-predictors.html#level-2-participant-level-indicator-of-sports-participation"><i class="fa fa-check"></i><b>10.6.2</b> Level-2: Participant-level indicator of sports participation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multilevel-logistic-regression.html"><a href="multilevel-logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Multilevel logistic regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="multilevel-logistic-regression.html"><a href="multilevel-logistic-regression.html#unconditional-model"><i class="fa fa-check"></i><b>11.1</b> Unconditional model</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="multilevel-logistic-regression.html"><a href="multilevel-logistic-regression.html#fitting-the-model"><i class="fa fa-check"></i><b>11.1.1</b> Fitting the model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="multilevel-logistic-regression.html"><a href="multilevel-logistic-regression.html#conditional-model"><i class="fa fa-check"></i><b>11.2</b> Conditional model</a></li>
<li class="chapter" data-level="11.3" data-path="multilevel-logistic-regression.html"><a href="multilevel-logistic-regression.html#interpreting-odds-ratios"><i class="fa fa-check"></i><b>11.3</b> Interpreting odds ratios</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="mlm-v.-gee-for-binary-outcomes.html"><a href="mlm-v.-gee-for-binary-outcomes.html"><i class="fa fa-check"></i><b>12</b> MLM v. GEE for Binary Outcomes</a>
<ul>
<li class="chapter" data-level="12.1" data-path="mlm-v.-gee-for-binary-outcomes.html"><a href="mlm-v.-gee-for-binary-outcomes.html#overview-3"><i class="fa fa-check"></i><b>12.1</b> Overview</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="mlm-v.-gee-for-binary-outcomes.html"><a href="mlm-v.-gee-for-binary-outcomes.html#dataset"><i class="fa fa-check"></i><b>12.1.1</b> Dataset</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="mlm-v.-gee-for-binary-outcomes.html"><a href="mlm-v.-gee-for-binary-outcomes.html#hlm-model"><i class="fa fa-check"></i><b>12.2</b> HLM Model</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="mlm-v.-gee-for-binary-outcomes.html"><a href="mlm-v.-gee-for-binary-outcomes.html#interpreting-odds-ratios-1"><i class="fa fa-check"></i><b>12.2.1</b> Interpreting odds ratios</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="mlm-v.-gee-for-binary-outcomes.html"><a href="mlm-v.-gee-for-binary-outcomes.html#population-average-model-with-gee"><i class="fa fa-check"></i><b>12.3</b> Population Average Model with GEE</a></li>
<li class="chapter" data-level="12.4" data-path="mlm-v.-gee-for-binary-outcomes.html"><a href="mlm-v.-gee-for-binary-outcomes.html#hlm-with-level-2-predictors"><i class="fa fa-check"></i><b>12.4</b> HLM with level-2 predictors</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="mlm-v.-gee-for-binary-outcomes.html"><a href="mlm-v.-gee-for-binary-outcomes.html#random-slopes-1"><i class="fa fa-check"></i><b>12.4.1</b> Random slopes</a></li>
<li class="chapter" data-level="12.4.2" data-path="mlm-v.-gee-for-binary-outcomes.html"><a href="mlm-v.-gee-for-binary-outcomes.html#level-2-predictors"><i class="fa fa-check"></i><b>12.4.2</b> Level 2 predictors</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="mlm-v.-gee-for-binary-outcomes.html"><a href="mlm-v.-gee-for-binary-outcomes.html#compare-to-gee"><i class="fa fa-check"></i><b>12.5</b> Compare to GEE</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="mlm-v.-gee-for-binary-outcomes.html"><a href="mlm-v.-gee-for-binary-outcomes.html#visualizing-res"><i class="fa fa-check"></i><b>12.5.1</b> Visualizing REs</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Generalized Linear Mixed Models with R: A tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="more-on-hierarchical-linear-models" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">8</span> More on Hierarchical Linear Models<a href="more-on-hierarchical-linear-models.html#more-on-hierarchical-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="overview-1" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Overview<a href="more-on-hierarchical-linear-models.html#overview-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In part 1, we covered the One-way ANOVA with random effects and Means as Outcomes models from Raudenbush and Bryk. This week, we cover the remaining models with an aim to reconstruct the results from pages 75-86 in their book.</p>
<p>This vignette makes use of some great code by <a href="https://www.rensvandeschoot.com/tutorials/lme4/">Rens van de Schoot</a></p>
<div id="load-the-data" class="section level3 hasAnchor" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Load the data<a href="more-on-hierarchical-linear-models.html#load-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ll continue using the HSB data set, loaded from the github repository</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="more-on-hierarchical-linear-models.html#cb248-1" tabindex="-1"></a>HSB <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="at">file =</span> <span class="fu">url</span>(<span class="st">&quot;https://rickhass.github.io/HSB_data.rds&quot;</span>))</span></code></pre></div>
</div>
</div>
<div id="random-coefficients-model" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Random Coefficients model<a href="more-on-hierarchical-linear-models.html#random-coefficients-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To remind you, this model is a two-level model, with a predictor at level 1 (group-centered-SES or <code>cses</code> in our data set), and the outcome also at level 1 (<code>mathach</code>).</p>
<p>The purpose of this model is truly multilevel: you want to know about an overall average effect of a level-1 predictor across groups, but also about how variable that effect is across the groups.</p>
<p>Here, we’re asking what is the average effect of <code>cses</code> on math achievement across schools, and by how much does it vary from school to school. Later, we’ll ask whether variables at the school level relate to the slope variation.</p>
<p>Level 1:</p>
<p><span class="math display">\[
Y_{ij} = \beta_{0j} + \beta_{1j}(cses) + r_{ij}
\]</span>
Level 2:
<span class="math display">\[
\begin{aligned}
\beta_{0j} = \gamma_{00} + u_{0j} \\
\beta_{1j} = \gamma_{11} + u_{1j}
\end{aligned}
\]</span></p>
<div id="visualizing-the-within-school-slopes" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Visualizing the within-school slopes<a href="more-on-hierarchical-linear-models.html#visualizing-the-within-school-slopes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To get a sense of what we’re doing, let’s first visualize the <code>mathach ~ cses</code> relationship without respect to school.</p>
<p>Below is an overall plot of all <span class="math inline">\(n = 7185\)</span> students’ <code>cses</code> and math achievement scores. It looks pretty incoherent. Given the known hierarchical structure of the data, this is not surprising, so we should try to understand what’s happening within each school and between schools.</p>
<p>However, we still see that, ignoring the clustering, there’s some degree of linear
relationship between a student’s SES and his or her math achievement score.</p>
<p><img src="_main_files/figure-html/plot108-1.png" width="672" /></p>
<p>Now, let’s zoom in on a few schools and see if there are some differences in the cses relationship across them. To make it easier to see, we’ll sample <span class="math inline">\(J = 40\)</span> schools and plot a regression line for each:</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="more-on-hierarchical-linear-models.html#cb249-1" tabindex="-1"></a>schools <span class="ot">&lt;-</span> <span class="fu">sample</span>(HSB<span class="sc">$</span>school, <span class="dv">40</span>)</span>
<span id="cb249-2"><a href="more-on-hierarchical-linear-models.html#cb249-2" tabindex="-1"></a></span>
<span id="cb249-3"><a href="more-on-hierarchical-linear-models.html#cb249-3" tabindex="-1"></a>Sampledata <span class="ot">&lt;-</span> HSB[<span class="fu">is.element</span>(HSB<span class="sc">$</span>school,schools), ]</span>
<span id="cb249-4"><a href="more-on-hierarchical-linear-models.html#cb249-4" tabindex="-1"></a></span>
<span id="cb249-5"><a href="more-on-hierarchical-linear-models.html#cb249-5" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> Sampledata, <span class="fu">aes</span>(<span class="at">x =</span> cses, <span class="at">y =</span> mathach, <span class="at">col =</span> school, <span class="at">group =</span> school)) <span class="sc">+</span></span>
<span id="cb249-6"><a href="more-on-hierarchical-linear-models.html#cb249-6" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">alpha =</span> .<span class="dv">7</span>, <span class="at">position =</span> <span class="st">&quot;jitter&quot;</span>) <span class="sc">+</span></span>
<span id="cb249-7"><a href="more-on-hierarchical-linear-models.html#cb249-7" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb249-8"><a href="more-on-hierarchical-linear-models.html#cb249-8" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb249-9"><a href="more-on-hierarchical-linear-models.html#cb249-9" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm,</span>
<span id="cb249-10"><a href="more-on-hierarchical-linear-models.html#cb249-10" tabindex="-1"></a>              <span class="at">se     =</span> <span class="cn">FALSE</span>,</span>
<span id="cb249-11"><a href="more-on-hierarchical-linear-models.html#cb249-11" tabindex="-1"></a>              <span class="at">linewidth   =</span> .<span class="dv">4</span>, </span>
<span id="cb249-12"><a href="more-on-hierarchical-linear-models.html#cb249-12" tabindex="-1"></a>              <span class="at">alpha  =</span> .<span class="dv">7</span>) <span class="sc">+</span> <span class="co"># to add regression line</span></span>
<span id="cb249-13"><a href="more-on-hierarchical-linear-models.html#cb249-13" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title    =</span> <span class="st">&quot;SES and Math Achievement in a sample of 40 schools&quot;</span>,</span>
<span id="cb249-14"><a href="more-on-hierarchical-linear-models.html#cb249-14" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;The within-school regression lines&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/within-school08-1.png" width="672" /></p>
<p>We can see that most of the lines have positive slope, but there are a few that do not!</p>
<p>The random coefficients model focuses on two things:</p>
<ul>
<li>An overall average estimate across these regression equations
<ul>
<li><span class="math inline">\(\gamma_{00} =\)</span> weighted average of <span class="math inline">\(\beta_{0j}\)</span> the school-level regression intercepts</li>
<li><span class="math inline">\(\gamma_{10} =\)</span> the weighted average of <span class="math inline">\(\beta_{1j}\)</span> the school-level slopes for the <code>cses</code> ~ <code>mathach</code> relationship</li>
</ul></li>
<li>The estimated variability across schools in <span class="math inline">\(\beta_{0j}\)</span> <em>and</em> <span class="math inline">\(\beta_{1j}\)</span>
<ul>
<li>Since our equations for the gammas include a random effect of each school, we
can obtain variability estimates by working with these random effects <span class="math inline">\(u_{0j}\)</span> and <span class="math inline">\(u_{1j}\)</span></li>
</ul></li>
</ul>
<p><strong>Note</strong>: we’re not attempting to “explain” why schools have different intercepts and slopes,
so <span class="math inline">\(u_{0j}\)</span> and <span class="math inline">\(u_{1j}\)</span> for each school are simply the amount by which that school differs from the mean intercept (<span class="math inline">\(\gamma_{00}\)</span>) and mean slope (<span class="math inline">\(\gamma_{01}\)</span>), respectively. They’re essentially level-2 error terms.</p>
</div>
<div id="fitting-the-random-coefficients-2-level-model" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Fitting the Random Coefficients 2-level model<a href="more-on-hierarchical-linear-models.html#fitting-the-random-coefficients-2-level-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Substituting the leve-2 equations above into the level 1 equation with <code>cses</code> as the level-1 predictor, <code>mathach</code> as outcome, and <strong>both</strong> random intercepts (<span class="math inline">\(\beta_{0j}\)</span>) and slopes (<span class="math inline">\(\beta_{1j}\)</span>) we get:</p>
<p><span class="math display">\[
Y_{ij} = \gamma_{00} + \gamma_{10}(cses) + u_{0j} + u_{1j}(cses) + r_{ij}
\]</span></p>
<p>So that means, our mixed-effects output should contain estimates of 2 fixed effects:</p>
<ul>
<li><span class="math inline">\(\gamma_{00}\)</span>, the weighted average intercept across schools (scaled to be the grand mean)</li>
<li><span class="math inline">\(\gamma_{10}\)</span>, the weighted average <code>cses</code>-<code>mathach</code> slope across schools</li>
</ul>
<p>Note that we do not estimate <span class="math inline">\(\beta_{0j}\)</span> or <span class="math inline">\(\beta_{1j}\)</span> directly, but they can be computed. Generally, that’s not our goal, but we could use these estimates for data in other analyses</p>
<p>We get estimates of the <strong>variances</strong> of 3 random effects</p>
<ul>
<li><span class="math inline">\(\text{Var}(u_{0j}) = \tau_{00}\)</span></li>
<li><span class="math inline">\(\text{Var}(u_{1j}) = \tau_{11}\)</span></li>
<li><span class="math inline">\(\text{Var}(r_{ij}) = \sigma^{2}\)</span></li>
</ul>
<p>AND we have a new term, a <strong>covariance</strong> between <span class="math inline">\(u_{0j}\)</span> and <span class="math inline">\(u_{1j}\)</span></p>
<ul>
<li><span class="math inline">\(\text{Cov}(u_{0j},u_{1j}) = \tau_{10}\)</span></li>
</ul>
<p>Let’s now estimate our mixed-effects parameters</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="more-on-hierarchical-linear-models.html#cb250-1" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">lmer</span>(mathach <span class="sc">~</span> cses <span class="sc">+</span> (cses<span class="sc">|</span>school), <span class="at">data =</span> HSB)</span>
<span id="cb250-2"><a href="more-on-hierarchical-linear-models.html#cb250-2" tabindex="-1"></a></span>
<span id="cb250-3"><a href="more-on-hierarchical-linear-models.html#cb250-3" tabindex="-1"></a><span class="fu">summary</span>(model3)</span></code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: mathach ~ cses + (cses | school)
##    Data: HSB
## 
## REML criterion at convergence: 46714.2
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -3.09680 -0.73193  0.01855  0.75386  2.89924 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  school   (Intercept)  8.681   2.9464       
##           cses         0.694   0.8331   0.02
##  Residual             36.700   6.0581       
## Number of obs: 7185, groups:  school, 160
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  12.6362     0.2445   51.68
## cses          2.1932     0.1283   17.10
## 
## Correlation of Fixed Effects:
##      (Intr)
## cses 0.009</code></pre>
<p>For inference, we’ll use bootstrapped confidence intervals:</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="more-on-hierarchical-linear-models.html#cb252-1" tabindex="-1"></a><span class="fu">confint</span>(model3, <span class="at">method =</span> <span class="st">&quot;boot&quot;</span>)</span></code></pre></div>
<pre><code>##                  2.5 %     97.5 %
## .sig01       2.5968018  3.2927604
## .sig02      -0.3022129  0.4414194
## .sig03       0.4346349  1.1285269
## .sigma       5.9544060  6.1672046
## (Intercept) 12.1208481 13.1124875
## cses         1.9523704  2.4369979</code></pre>
<p>Here’s a graph showing the difference between a hierarchical model and an OLS regression line.</p>
<p><img src="_main_files/figure-html/compare_lines08-1.png" width="672" /></p>
</div>
<div id="adding-additional-level-1-predictors-to-the-random-coefficients-model" class="section level3 hasAnchor" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Adding additional level-1 predictors to the random coefficients model<a href="more-on-hierarchical-linear-models.html#adding-additional-level-1-predictors-to-the-random-coefficients-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can add as many level-1 predictors as we like. If we want to <strong>also model the level-1 slopes for these predictors as random</strong>, we need to change how our random-effects syntax looks. It is very similar to the combined equation though:</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="more-on-hierarchical-linear-models.html#cb254-1" tabindex="-1"></a>mod3a <span class="ot">&lt;-</span> <span class="fu">lmer</span>(mathach <span class="sc">~</span> cses <span class="sc">+</span> minority <span class="sc">+</span> (cses <span class="sc">+</span> minority <span class="sc">|</span> school), <span class="at">data =</span>HSB)</span></code></pre></div>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<p>We may have a <code>singular fit</code> issue, but we’ll ignore it. This happens when variance components are close to zero, the boundary of the parameter space.</p>
<p>The combined equation from the above model is:</p>
<p>Model equation:
<span class="math display">\[
Y_{ij} = \gamma_{00} + \gamma_{10}(cses) + \gamma_{20}(minority = \text{Yes}) + u_{0j} + u_{1j}(cses) + u_{2j}(sex) + r_{ij}
\]</span></p>
<p>Here’s our results. Note the new rows in the Random Effects part of the output and how small the <code>cses</code> variance is. The correlation between <code>cses</code>slopes and the school <code>(Intercept)</code> is also estimated to be <span class="math inline">\(-1.0\)</span> which is likely another cause of the warning.</p>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: mathach ~ cses + minority + (cses + minority | school)
##    Data: HSB
## 
## REML criterion at convergence: 46503.4
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -3.13665 -0.71773  0.03569  0.75813  2.98894 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr       
##  school   (Intercept)  5.8580  2.4203              
##           cses         0.0114  0.1068   -1.00      
##           minorityYes  1.9527  1.3974    0.30 -0.30
##  Residual             35.9143  5.9929              
## Number of obs: 7185, groups:  school, 160
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  13.5218     0.2134   63.35
## cses          1.9047     0.1092   17.44
## minorityYes  -3.1596     0.2515  -12.56
## 
## Correlation of Fixed Effects:
##             (Intr) cses  
## cses        -0.111       
## minorityYes -0.108  0.123
## optimizer (nloptwrap) convergence code: 0 (OK)
## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<p>Not only do we have a new variance (<span class="math inline">\(\text{Var}(u_{2j}) = \tau_{22}\)</span>), we have two new covariances, in fact, we have a 3 by 3 <em>variance-covariance matrix</em> <span class="math inline">\(\textbf{T}\)</span>:</p>
<p><span class="math display">\[
\begin{bmatrix}
  \tau_{00}   &amp;.    &amp;. \\
  \tau_{10}   &amp;\tau_{11}    &amp;. \\
  \tau_{20}   &amp;\tau_{21}    &amp;\tau_{22}
\end{bmatrix}
\]</span>
The lower triangle (including the diagonal terms) are the only unique terms here. The diagonal lists the 3 random effect variances (of the random intercept, cses slope, and sex slopes in that order). The other entries are the covariances between those 3 terms. For example, <span class="math inline">\(\tau_{10}\)</span> is the covariance between the random effect <span class="math inline">\(u_{0j}\)</span> and <span class="math inline">\(u_{1j}\)</span></p>
<p>In the output from r, we only get this lower triangle including <span class="math inline">\(\tau_{10}\)</span>, <span class="math inline">\(\tau_{20}\)</span>, and <span class="math inline">\(\tau_{21}\)</span>. The variances are given in the variance column, along with <span class="math inline">\(\sigma^{2}\)</span> which is the variance of the level-1 residuals (<span class="math inline">\(r_{ij}\)</span>)</p>
<p>Finally, we can ask how much variance at level-1 we’ve reduced by adding <code>minority</code> as
a level 1 predictor. We can use the CSES model as the comparison.</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="more-on-hierarchical-linear-models.html#cb257-1" tabindex="-1"></a>(<span class="fl">36.700</span> <span class="sc">-</span> <span class="fl">35.91</span>) <span class="sc">/</span> <span class="fl">36.700</span></span></code></pre></div>
<pre><code>## [1] 0.02152589</code></pre>
<p>So despite the average slope for minority being significantly different from zero, we haven’t
explained much more variance by adding it.</p>
</div>
</div>
<div id="intercepts-and-slopes-as-outcomes" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Intercepts and Slopes as Outcomes<a href="more-on-hierarchical-linear-models.html#intercepts-and-slopes-as-outcomes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now illustrate the Intercepts-and-slopes-as-outcomes model. We will fit exactly what appears in Raudenbush &amp; Bryk, Chapter 4, but note, we can include more predictors at each level if we wish.</p>
<p>Let’s look at the hierarchical model:</p>
<ul>
<li>Level 1
<ul>
<li><span class="math inline">\(Y_{ij} = \beta_{0j} + \beta_{1j}(cses_{ij}) + r_{ij}\)</span></li>
</ul></li>
<li>Level 2
<ul>
<li><span class="math inline">\(\beta_{0j} = \gamma_{00} + \gamma_{01}(mean.ses_{j}) + \gamma_{02}(Sector_{j}) + u_{0j}\)</span></li>
<li><span class="math inline">\(\beta_{1j} = \gamma_{10} + \gamma_{11}(mean.ses_{j}) + \gamma_{12}(Sector_{j}) + u_{1j}\)</span></li>
</ul></li>
</ul>
<p>So we know we have 3 predictor variables:</p>
<ol style="list-style-type: decimal">
<li><code>cses</code> at level 1</li>
<li><code>mean.ses</code> at level 2</li>
<li><code>Sector</code>, which is an indicator for <code>Catholic</code> at level 2</li>
</ol>
<div id="interactions-and-the-iso-model" class="section level3 hasAnchor" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> Interactions and the ISO model<a href="more-on-hierarchical-linear-models.html#interactions-and-the-iso-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The combined model for the above includes interactions. This is due to the fact that when we
substitute the level 2 equation for <span class="math inline">\(\beta_{1j}\)</span> into the level 1 equation, the
entire level 2 equation is multiplied by <span class="math inline">\(cses_{ij}\)</span> so we have:</p>
<p><span class="math display">\[
Y_{ij} = [\gamma_{00} + \gamma_{01}(mean.ses_{j}) + \gamma_{02}(Sector_{j}) + u_{0j}] + [cses \times (\gamma_{10} + \gamma_{11}(mean.ses_{j}) + \gamma_{12}(Sector_{j}) + u_{1j})] + r_{ij}
\]</span></p>
<p>In the first bracket above, we have the substitution of <span class="math inline">\(\beta_{0j}\)</span> with the level-2 equation for it.
Since <span class="math inline">\(\beta_{0j}\)</span> is the intercept, and not multiplied by anything, these terms just add right in.</p>
<p>However, in the second set of brackets, we see that the equation for <span class="math inline">\(\beta_{1j}\)</span> gets multiplied by the level-1 predictor <span class="math inline">\(cses_{ij}\)</span> because that’s how it’s written at level 1. Multiplying through we have</p>
<p><span class="math display">\[
Y_{ij} = [\gamma_{00} + \gamma_{01}(mean.ses_{j}) + \gamma_{02}(Sector_{j}) + u_{0j}] + [(cses \times\gamma_{10}) + (cses \times \gamma_{11}(mean.ses_{j})) + (cses \times \gamma_{12}(Sector_{j}))+ (cses \times u_{1j})] + r_{ij}
\]</span></p>
<p>Rearranging our terms we see that the first 4 fixed effects (including the intercept) are
<strong>not</strong> interactions, the next 2 are, and then we have the random part. Note that in the random part
there is an interaction between <span class="math inline">\(cses\)</span> and <span class="math inline">\(u_{ij}\)</span> but <code>lmer</code> specifies this for us.</p>
<p><span class="math display">\[
Y_{ij} = \gamma_{00} + \gamma_{01}(mean.ses_{j}) + \gamma_{02}(Sector_{j}) + \gamma_{10}(cses) + \gamma_{11}(mean.ses_{j} \times cses) + \gamma_{12}(Sector_{j} \times cses)+ u_{0j} + u_{1j}(cses) + r_{ij}
\]</span></p>
<p>So we insert the fixed effects into the <code>lmer</code> formula, and then specify random <code>cses</code> slopes
as with the random coefficients model <code>(cses|school)</code> and <code>lmer</code> takes care of the rest.</p>
<p><strong>Note:</strong> You can use <code>*</code> or <code>:</code> for interaction terms, but the <code>*</code> is better here as it will build the necessary terms for you without duplication or omission. Using <code>:</code> you would need to specify each term in the equation. See below (<code>mod4alt</code>):</p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="more-on-hierarchical-linear-models.html#cb259-1" tabindex="-1"></a><span class="co"># using the * operator for interactions</span></span>
<span id="cb259-2"><a href="more-on-hierarchical-linear-models.html#cb259-2" tabindex="-1"></a>mod4 <span class="ot">&lt;-</span> <span class="fu">lmer</span>(mathach <span class="sc">~</span> cses<span class="sc">*</span>mean.ses <span class="sc">+</span> cses<span class="sc">*</span>sector <span class="sc">+</span> (cses <span class="sc">|</span> school), <span class="at">data =</span> HSB)</span>
<span id="cb259-3"><a href="more-on-hierarchical-linear-models.html#cb259-3" tabindex="-1"></a></span>
<span id="cb259-4"><a href="more-on-hierarchical-linear-models.html#cb259-4" tabindex="-1"></a><span class="co"># alternate specification forming interaction terms &quot;by hand&quot; with &quot;:&quot;</span></span>
<span id="cb259-5"><a href="more-on-hierarchical-linear-models.html#cb259-5" tabindex="-1"></a>mod4alt <span class="ot">&lt;-</span> <span class="fu">lmer</span>(mathach <span class="sc">~</span> cses <span class="sc">+</span> mean.ses <span class="sc">+</span> sector <span class="sc">+</span> cses<span class="sc">:</span>mean.ses <span class="sc">+</span> cses<span class="sc">:</span>sector <span class="sc">+</span> </span>
<span id="cb259-6"><a href="more-on-hierarchical-linear-models.html#cb259-6" tabindex="-1"></a>                  (cses<span class="sc">|</span>school), <span class="at">data =</span> HSB)</span></code></pre></div>
<p>In the output, we can see that, indeed, we have the correct interaction variables in the model. Now, what do they mean?</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="more-on-hierarchical-linear-models.html#cb260-1" tabindex="-1"></a><span class="fu">summary</span>(mod4)</span></code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: mathach ~ cses * mean.ses + cses * sector + (cses | school)
##    Data: HSB
## 
## REML criterion at convergence: 46503.7
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -3.15926 -0.72319  0.01704  0.75444  2.95822 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  school   (Intercept)  2.380   1.5426       
##           cses         0.101   0.3179   0.39
##  Residual             36.721   6.0598       
## Number of obs: 7185, groups:  school, 160
## 
## Fixed effects:
##                     Estimate Std. Error t value
## (Intercept)          12.1279     0.1993  60.856
## cses                  2.9450     0.1556  18.928
## mean.ses              5.3329     0.3692  14.446
## sectorCatholic        1.2266     0.3063   4.005
## cses:mean.ses         1.0393     0.2989   3.477
## cses:sectorCatholic  -1.6427     0.2398  -6.851
## 
## Correlation of Fixed Effects:
##             (Intr) cses   men.ss sctrCt css:m.
## cses         0.075                            
## mean.ses     0.256  0.019                     
## sectorCthlc -0.699 -0.053 -0.356              
## cses:men.ss  0.019  0.293  0.074 -0.026       
## css:sctrCth -0.052 -0.696 -0.027  0.077 -0.351</code></pre>
</div>
<div id="mean-ses-and-student-ses" class="section level3 hasAnchor" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Mean SES and Student SES<a href="more-on-hierarchical-linear-models.html#mean-ses-and-student-ses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>cses:mean.ses</code> term in the model is positive, and has a t-value that is likely significant (<code>3.477</code>). This means that the student ses relation to math achievement gets more positive as the school’s average SES increases. So schools with a higher mean SES tend to have stronger student-level ses effects on math achievement. This is true because the marginal <code>cses</code> slope is positive, so the positive interaction term adds to the already positive slope.</p>
</div>
<div id="sector-and-student-ses" class="section level3 hasAnchor" number="8.3.3">
<h3><span class="header-section-number">8.3.3</span> Sector and Student SES<a href="more-on-hierarchical-linear-models.html#sector-and-student-ses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In contrast, the <code>cses:sectorCatholic</code> term in the model is negative, but with a large t-value, which is also likely significant. It tells us that on average, Catholic schools have slopes that are -1.6427 less than Public schools. So the effect of student-level ses on math achievement is weaker in Public Schools, but still net positive on average.</p>
</div>
<div id="visualizing-the-mean-ses-effect" class="section level3 hasAnchor" number="8.3.4">
<h3><span class="header-section-number">8.3.4</span> Visualizing the mean SES effect<a href="more-on-hierarchical-linear-models.html#visualizing-the-mean-ses-effect" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To understand the interaction between mean.ses and cses, it’s helpful to split Mean SES into categories and plot the different schools. We do this using the <code>cut</code> command.</p>
<p>In <code>cut</code> we specify <code>breaks = 3</code> to just let R pick 3 equally spaced categories that we name
low, average, and high ses. There are better ways to do it, but this is a nice quick and dirty method</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="more-on-hierarchical-linear-models.html#cb262-1" tabindex="-1"></a>HSB<span class="sc">$</span>mean.ses.cat <span class="ot">&lt;-</span> <span class="fu">cut</span>(HSB<span class="sc">$</span>mean.ses,</span>
<span id="cb262-2"><a href="more-on-hierarchical-linear-models.html#cb262-2" tabindex="-1"></a>                        <span class="at">breaks =</span> <span class="dv">3</span>,</span>
<span id="cb262-3"><a href="more-on-hierarchical-linear-models.html#cb262-3" tabindex="-1"></a>                        <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&#39;lowSES&#39;</span>,<span class="st">&#39;averageSES&#39;</span>,<span class="st">&#39;highSES&#39;</span>))</span></code></pre></div>
<p>As before, we will use the <code>ggplot2</code> package to make the graphs. Consult <a href="https://r4ds.had.co.nz/data-visualisation.html">Hadley Wickham’s e-Book</a> Chapter 3 for an introduction to the syntax.</p>
<p>Note, I am colorblind so I’m using this (rather ugly) colorblind palette, which is
just a collection of hex codes for colors that have good contrast.</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="more-on-hierarchical-linear-models.html#cb263-1" tabindex="-1"></a>cbbPalette <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;#000000&quot;</span>, <span class="st">&quot;#E69F00&quot;</span>, <span class="st">&quot;#56B4E9&quot;</span>, <span class="st">&quot;#009E73&quot;</span>, <span class="st">&quot;#F0E442&quot;</span>, <span class="st">&quot;#0072B2&quot;</span>, <span class="st">&quot;#D55E00&quot;</span>, <span class="st">&quot;#CC79A7&quot;</span>)</span>
<span id="cb263-2"><a href="more-on-hierarchical-linear-models.html#cb263-2" tabindex="-1"></a></span>
<span id="cb263-3"><a href="more-on-hierarchical-linear-models.html#cb263-3" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> HSB, <span class="fu">aes</span>(<span class="at">x =</span> cses, <span class="at">y =</span> mathach, <span class="at">col =</span> mean.ses.cat, <span class="at">group =</span> school)) <span class="sc">+</span></span>
<span id="cb263-4"><a href="more-on-hierarchical-linear-models.html#cb263-4" tabindex="-1"></a>  <span class="co">#geom_point(size = 1, alpha = .7, position = &quot;jitter&quot;) +</span></span>
<span id="cb263-5"><a href="more-on-hierarchical-linear-models.html#cb263-5" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb263-6"><a href="more-on-hierarchical-linear-models.html#cb263-6" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> cbbPalette[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]) <span class="sc">+</span></span>
<span id="cb263-7"><a href="more-on-hierarchical-linear-models.html#cb263-7" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm,</span>
<span id="cb263-8"><a href="more-on-hierarchical-linear-models.html#cb263-8" tabindex="-1"></a>              <span class="at">se     =</span> <span class="cn">FALSE</span>,</span>
<span id="cb263-9"><a href="more-on-hierarchical-linear-models.html#cb263-9" tabindex="-1"></a>              <span class="at">linewidth   =</span> .<span class="dv">4</span>, </span>
<span id="cb263-10"><a href="more-on-hierarchical-linear-models.html#cb263-10" tabindex="-1"></a>              <span class="at">alpha  =</span> .<span class="dv">7</span>) <span class="sc">+</span> <span class="co"># to add regression line</span></span>
<span id="cb263-11"><a href="more-on-hierarchical-linear-models.html#cb263-11" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title    =</span> <span class="st">&quot;SES and Math Achievement&quot;</span>,</span>
<span id="cb263-12"><a href="more-on-hierarchical-linear-models.html#cb263-12" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;The within-school regression lines colored by MeanSES&quot;</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/plotmean.ses-1.png" width="672" /></p>
<p>As can be seen, there is a clear difference in the slopes across the 3 categories of Mean SES. Note that in this way, we’re really seeing the multilevel part in action. We have 2 sample levels, one of the 7185 students, and now we can see that the 160 schools make up a second (random) sample. This random sample has systematic components, one of them being Mean SES. But instead of just looking at the relation between this level-2 systematic component and a measurement, we looking at the relationship between the level-2 systematic component and the strength of a level-1 relationship.</p>
</div>
</div>
<div id="model-comparison-with-likelihood-ratios-fixed-and-random" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Model comparison with Likelihood Ratios: Fixed and Random<a href="more-on-hierarchical-linear-models.html#model-comparison-with-likelihood-ratios-fixed-and-random" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As noted in lecture, there are some issues with p-values from likelihood ratio tests (LRTs) when we have dependent data:</p>
<ol style="list-style-type: decimal">
<li>The LRTs for fixed effects (using <code>REML = FALSE</code>) can be <strong>too small</strong> and inflate Type I error</li>
<li>The LRT for random effects (using <code>REML = TRUE</code>) can be <strong>too big</strong> inflating Type II error</li>
</ol>
<p>These are really only issues for model building: deciding on the fixed or random components of your final model. In the final model, you can use the bootstrapped confidence intervals</p>
<div id="example-1-lrt-for-fixed-effects" class="section level3 hasAnchor" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> Example 1: LRT for fixed effects<a href="more-on-hierarchical-linear-models.html#example-1-lrt-for-fixed-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s say we want a p-value for the <code>minority</code> effect in model 3a. We could fit a model without <code>minority</code> and use <code>anova</code> to compute the LRT <span class="math inline">\(p\)</span>-value, but we’ll run into two problems</p>
<ol style="list-style-type: decimal">
<li>Model 3a has a random slope for <code>minority</code>, and so there is no way to fit a model without sex as a predictor, but allowing for random slopes</li>
<li>We may find the <span class="math inline">\(p\)</span> value is too small since the <span class="math inline">\(\chi^{2}\)</span> approximation is not exact</li>
</ol>
<p>Let’s see what happens in a simpler situation: a random intercept model</p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="more-on-hierarchical-linear-models.html#cb265-1" tabindex="-1"></a>mod3_int <span class="ot">&lt;-</span> <span class="fu">lmer</span>(mathach <span class="sc">~</span> cses <span class="sc">+</span> minority <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> school), <span class="at">data =</span>HSB)</span>
<span id="cb265-2"><a href="more-on-hierarchical-linear-models.html#cb265-2" tabindex="-1"></a>mod3_int_null <span class="ot">&lt;-</span> <span class="fu">lmer</span>(mathach <span class="sc">~</span> cses <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> school), <span class="at">data =</span>HSB)</span>
<span id="cb265-3"><a href="more-on-hierarchical-linear-models.html#cb265-3" tabindex="-1"></a></span>
<span id="cb265-4"><a href="more-on-hierarchical-linear-models.html#cb265-4" tabindex="-1"></a><span class="fu">anova</span>(mod3_int_null, mod3_int)</span></code></pre></div>
<pre><code>## refitting model(s) with ML (instead of REML)</code></pre>
<pre><code>## Data: HSB
## Models:
## mod3_int_null: mathach ~ cses + (1 | school)
## mod3_int: mathach ~ cses + minority + (1 | school)
##               npar   AIC   BIC logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)    
## mod3_int_null    4 46728 46756 -23360     46720                         
## mod3_int         5 46523 46557 -23256     46513 207.34  1  &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>There is a significant reduction in the deviance for the model with <code>minority</code> but it may not be exact. Below is the code to run a parametric bootstrapped likelihood ratio test. Note, it can take quite a while to run (and actually prints out how long in the output):</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="more-on-hierarchical-linear-models.html#cb268-1" tabindex="-1"></a>pbkrtest<span class="sc">::</span><span class="fu">PBmodcomp</span>(mod3_int, mod3_int_null)</span></code></pre></div>
<pre><code>## Bootstrap test; time: 42.41 sec; samples: 1000; extremes: 0;
## large : mathach ~ cses + minority + (1 | school)
##          stat df   p.value    
## LRT    207.34  1 &lt; 2.2e-16 ***
## PBtest 207.34     0.000999 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>So this tells us that under a null model, there only <span class="math inline">\(0.09 \%\)</span> of samples returned LRT statistics greater than <span class="math inline">\(207.34\)</span>. That means that our <span class="math inline">\(LR = 207.34\)</span> is <strong>NOT</strong> compatible with the null, so we should reject it: <code>minority</code> <strong>IS</strong> significant.</p>
</div>
<div id="example-2-lrt-for-random-effects" class="section level3 hasAnchor" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span> Example 2: LRT for random effects<a href="more-on-hierarchical-linear-models.html#example-2-lrt-for-random-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similarly, we might want to test if the variance of the <code>cses</code> slopes is greater than 0. <code>model3</code> is the full model, and we fit a reduced model and test it:</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="more-on-hierarchical-linear-models.html#cb270-1" tabindex="-1"></a>mod3_null <span class="ot">&lt;-</span> <span class="fu">lmer</span>(mathach <span class="sc">~</span> cses <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span> school), <span class="at">data =</span> HSB)</span>
<span id="cb270-2"><a href="more-on-hierarchical-linear-models.html#cb270-2" tabindex="-1"></a></span>
<span id="cb270-3"><a href="more-on-hierarchical-linear-models.html#cb270-3" tabindex="-1"></a><span class="fu">anova</span>(model3, mod3_null, <span class="at">refit =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## Data: HSB
## Models:
## mod3_null: mathach ~ cses + (1 | school)
## model3: mathach ~ cses + (cses | school)
##           npar   AIC   BIC logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)   
## mod3_null    4 46732 46760 -23362     46724                        
## model3       6 46726 46768 -23357     46714 9.7617  2   0.007591 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Again, we see evidence of a significant reduction in deviance when we allow for random slopes. Again, this can be too conservative. We omit the <code>pbkrtest</code> results since they take quite a while and have some convergence issues due to the small scale of the variance in the <span class="math inline">\(\beta\)</span>’s for cses across schools.</p>
<p>What you might do in this situation is reason that the risk in testing random effects is that the p-value is conservative, and since it’s still well below <span class="math inline">\(\alpha = 0.05\)</span> you might accept this result.</p>
</div>
</div>
<div id="diagnostics" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Diagnostics<a href="more-on-hierarchical-linear-models.html#diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In our final section, we will quickly illustrate how to produce some residual plots to examine whether assumptions have been violated.</p>
<p>Our aim is to simply examine the residuals to see if they indeed follow a normal distribution. We do this at <em>both</em> levels:</p>
<ul>
<li>Level 1: <span class="math inline">\(r_{ij} \sim \mathcal{N}(0,\sigma^{2})\)</span></li>
<li>Level 2: <span class="math inline">\(u_{qj} \sim \mathcal{N}(0,\tau_{qq})\)</span></li>
</ul>
<p>We’ll look at model 3 with random intercepts and slopes for <code>cses</code></p>
<p>At level 1:</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="more-on-hierarchical-linear-models.html#cb272-1" tabindex="-1"></a><span class="do">## QQ plot of the level-1 residuals</span></span>
<span id="cb272-2"><a href="more-on-hierarchical-linear-models.html#cb272-2" tabindex="-1"></a><span class="fu">qqnorm</span>(<span class="fu">residuals</span>(model3))</span></code></pre></div>
<p><img src="_main_files/figure-html/resid108-1.png" width="672" /></p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="more-on-hierarchical-linear-models.html#cb273-1" tabindex="-1"></a><span class="do">## Scatter plot of level-1 residuals against predictions</span></span>
<span id="cb273-2"><a href="more-on-hierarchical-linear-models.html#cb273-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">fitted</span>(model3), <span class="fu">residuals</span>(model3))</span></code></pre></div>
<p><img src="_main_files/figure-html/resid108-2.png" width="672" /></p>
<p>At level 2, we will simply see if the distribution is Normal. We do this for both the random intercepts and random slopes</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="more-on-hierarchical-linear-models.html#cb274-1" tabindex="-1"></a>ranef3 <span class="ot">&lt;-</span> <span class="fu">ranef</span>(model3)</span>
<span id="cb274-2"><a href="more-on-hierarchical-linear-models.html#cb274-2" tabindex="-1"></a><span class="do">## QQ plot of the Intercept random effects</span></span>
<span id="cb274-3"><a href="more-on-hierarchical-linear-models.html#cb274-3" tabindex="-1"></a><span class="fu">qqnorm</span>(ranef3<span class="sc">$</span>school<span class="sc">$</span><span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/resid208-1.png" width="672" /></p>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="more-on-hierarchical-linear-models.html#cb275-1" tabindex="-1"></a><span class="do">## QQ plot of the Slope random effects</span></span>
<span id="cb275-2"><a href="more-on-hierarchical-linear-models.html#cb275-2" tabindex="-1"></a><span class="fu">qqnorm</span>(ranef3<span class="sc">$</span>school<span class="sc">$</span>cses)</span></code></pre></div>
<p><img src="_main_files/figure-html/resid208-2.png" width="672" /></p>
<p>We see in each case, we have nicely behaved results</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mixed-models-as-hierarchical-linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="using-lme4-to-fit-mlm-to-longitudinal-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
